{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Pulse Rate Algorithm\n",
    "\n",
    "### Contents\n",
    "Fill out this notebook as part of your final project submission.\n",
    "\n",
    "**You will have to complete both the Code and Project Write-up sections.**\n",
    "- The [Code](#Code) is where you will write a **pulse rate algorithm** and already includes the starter code.\n",
    "   - Imports - These are the imports needed for Part 1 of the final project. \n",
    "     - [glob](https://docs.python.org/3/library/glob.html)\n",
    "     - [numpy](https://numpy.org/)\n",
    "     - [scipy](https://www.scipy.org/)\n",
    "- The [Project Write-up](#Project-Write-up) to describe why you wrote the algorithm for the specific case.\n",
    "\n",
    "\n",
    "### Dataset\n",
    "You will be using the **Troika**[1] dataset to build your algorithm. Find the dataset under `datasets/troika/training_data`. The `README` in that folder will tell you how to interpret the data. The starter code contains a function to help load these files.\n",
    "\n",
    "1. Zhilin Zhang, Zhouyue Pi, Benyuan Liu, ‘‘TROIKA: A General Framework for Heart Rate Monitoring Using Wrist-Type Photoplethysmographic Signals During Intensive Physical Exercise,’’IEEE Trans. on Biomedical Engineering, vol. 62, no. 2, pp. 522-531, February 2015. Link\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.io\n",
    "import scipy.signal as sig\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import KFold,train_test_split \n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from tqdm import tqdm \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def LoadTroikaDataset():\n",
    "    \"\"\"\n",
    "    Retrieve the .mat filenames for the troika dataset.\n",
    "\n",
    "    Review the README in ./datasets/troika/ to understand the organization of the .mat files.\n",
    "\n",
    "    Returns:\n",
    "        data_fls: Names of the .mat files that contain signal data\n",
    "        ref_fls: Names of the .mat files that contain reference data\n",
    "        <data_fls> and <ref_fls> are ordered correspondingly, so that ref_fls[5] is the \n",
    "            reference data for data_fls[5], etc...\n",
    "    \"\"\"\n",
    "    data_dir = \"./datasets/troika/training_data\"\n",
    "    data_fls = sorted(glob.glob(data_dir + \"/DATA_*.mat\"))\n",
    "    ref_fls = sorted(glob.glob(data_dir + \"/REF_*.mat\"))\n",
    "    return data_fls, ref_fls\n",
    "\n",
    "def LoadTroikaDataFile(data_fl):\n",
    "    \"\"\"\n",
    "    Loads and extracts signals from a troika data file.\n",
    "\n",
    "    Usage:\n",
    "        data_fls, ref_fls = LoadTroikaDataset()\n",
    "        ppg, accx, accy, accz = LoadTroikaDataFile(data_fls[0])\n",
    "\n",
    "    Args:\n",
    "        data_fl: (str) filepath to a troika .mat file.\n",
    "\n",
    "    Returns:\n",
    "        numpy arrays for ppg, accx, accy, accz signals.\n",
    "    \"\"\"\n",
    "    data = sp.io.loadmat(data_fl)['sig']\n",
    "    return data[2:]\n",
    "\n",
    "\n",
    "\n",
    "def LoadTroikaRefFile(ref_file):\n",
    "    data = sp.io.loadmat(ref_file)['BPM0']\n",
    "    return data\n",
    "\n",
    "def BandpassFilter(signal, pass_band=(40/60,240/60), fs=125):\n",
    "      \n",
    "    b, a = sp.signal.butter(2, pass_band, btype='bandpass', fs = fs)\n",
    "    return sp.signal.filtfilt(b, a, signal)\n",
    "\n",
    "\n",
    "def AggregateErrorMetric(pr_errors, confidence_est):\n",
    "    \"\"\"\n",
    "    Computes an aggregate error metric based on confidence estimates.\n",
    "\n",
    "    Computes the MAE at 90% availability. \n",
    "\n",
    "    Args:\n",
    "        pr_errors: a numpy array of errors between pulse rate estimates and corresponding \n",
    "            reference heart rates.\n",
    "        confidence_est: a numpy array of confidence estimates for each pulse rate\n",
    "            error.\n",
    "\n",
    "    Returns:\n",
    "        the MAE at 90% availability\n",
    "    \"\"\"\n",
    "    # Higher confidence means a better estimate. The best 90% of the estimates\n",
    "    #    are above the 10th percentile confidence.\n",
    "    percentile90_confidence = np.percentile(confidence_est, 10)\n",
    "\n",
    "    # Find the errors of the best pulse rate estimates\n",
    "    best_estimates = pr_errors[confidence_est >= percentile90_confidence]\n",
    "\n",
    "    # Return the mean absolute error\n",
    "    return np.mean(np.abs(best_estimates))\n",
    "\n",
    "def Evaluate():\n",
    "    \"\"\"\n",
    "    Top-level function evaluation function.\n",
    "\n",
    "    Runs the pulse rate algorithm on the Troika dataset and returns an aggregate error metric.\n",
    "\n",
    "    Returns:\n",
    "        Pulse rate error on the Troika dataset. See AggregateErrorMetric.\n",
    "    \"\"\"\n",
    "    # Retrieve dataset files\n",
    "    data_fls, ref_fls = LoadTroikaDataset()\n",
    "    errs, confs = [], []\n",
    "    for data_fl, ref_fl in zip(data_fls, ref_fls):\n",
    "        # Run the pulse rate algorithm on each trial in the dataset\n",
    "        errors, confidence = RunPulseRateAlgorithm(data_fl, ref_fl)\n",
    "        errs.append(errors)\n",
    "        confs.append(confidence)\n",
    "        # Compute aggregate error metric\n",
    "    errs = np.hstack(errs)\n",
    "    confs = np.hstack(confs)\n",
    "    return AggregateErrorMetric(errs, confs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_features(ppg, accx, accy, accz, fs = 125):   \n",
    "    \"\"\"\n",
    "    Create features from the data\n",
    "    \n",
    "    Returns:\n",
    "        PPG and ACC features\n",
    "    \"\"\"\n",
    "    \n",
    "    mn_x = np.mean(accx)\n",
    "    std_x = np.std(accx)\n",
    "    energy_x = np.sum(np.square(accx - np.mean(accx)))\n",
    "    \n",
    "    mn_y = np.mean(accy)\n",
    "    std_y = np.std(accy)\n",
    "    energy_y = np.sum(np.square(accy - np.mean(accy)))\n",
    "    \n",
    "    mn_z = np.mean(accz)\n",
    "    std_z = np.std(accz)\n",
    "    energy_z = np.sum(np.square(accz - np.mean(accz)))\n",
    "                   \n",
    "    corr_xy = sp.stats.pearsonr(accx, accy)[0]\n",
    "    \n",
    "    mn_ppg = np.mean(ppg)\n",
    "    std_ppg = np.std(ppg)\n",
    "    \n",
    "                      \n",
    "                      \n",
    "                      \n",
    "    # magnitude of accelometer\n",
    "    acc=np.sqrt(accx**2 + accy**2 + accz**2)\n",
    "    # frequencies and coefficients for both ppg and acceleormeter magnitude \n",
    "    fft_acc=np.fft.rfft(acc,6*len(acc))\n",
    "    freqs_acc=np.fft.rfftfreq(6*len(acc),1/fs)\n",
    "    fft_ppg=np.fft.rfft(ppg,6*len(ppg))\n",
    "    freqs_ppg=np.fft.rfftfreq(6*len(ppg),1/fs)\n",
    "    #bandpass \n",
    "    fft_ppg[freqs_ppg <= 40/60] = 0.0\n",
    "    fft_ppg[freqs_ppg >= 240/60] = 0.0\n",
    "        \n",
    "    # getting peaks from the largest frequencies \n",
    "    #1) If the ppg peaks does not match with the acc peaks, have that in your result.\n",
    "    #2) If the ppg peaks match with the acc peaks, \n",
    "    #look for the second max peak of ppg.\n",
    "    #3) Sometimes, the second peak of ppg also match with the acc peak, \n",
    "    #in such case have that in your result.\n",
    "    max_acc_freq=np.argmax(np.abs(fft_acc))\n",
    "    max_ppg_freq=np.argmax(np.abs(fft_ppg))\n",
    "    acc_peak=freqs_acc[max_acc_freq]\n",
    "    ppg_peak=freqs_ppg[max_ppg_freq]\n",
    "    if acc_peak==ppg_peak:\n",
    "        fft_ppg=np.delete(fft_ppg,max_ppg_freq)\n",
    "        freqs_ppg=np.delete(freqs_ppg,max_ppg_freq)\n",
    "        max_acc_freq=np.argmax(np.abs(fft_acc))\n",
    "        max_ppg_freq=np.argmax(np.abs(fft_ppg))\n",
    "        acc_peak=freqs_acc[max_acc_freq]\n",
    "        ppg_peak=freqs_ppg[max_ppg_freq]\n",
    "    return np.array([ppg_peak, acc_peak,mn_x,std_x,energy_x,\n",
    "                    mn_y,std_y,energy_y,mn_z,std_z,energy_z,corr_xy,mn_ppg,std_ppg])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def ensemble_model():       \n",
    "    \"\"\"\n",
    "    Train Regressor model\n",
    "    \n",
    "    Returns:\n",
    "        Trained model\n",
    "    \"\"\" \n",
    "    fs = 125\n",
    "    window_length = 8\n",
    "    window_shift = 2\n",
    "    \n",
    "    # Load filenames through LoadTroikaDataset\n",
    "    data_fls, ref_fls = LoadTroikaDataset()\n",
    "    features, labels, signals = [], [], []\n",
    "    \n",
    "    for data_fl, ref_fl in zip(data_fls, ref_fls):\n",
    "        ppg, accx, accy, accz = LoadTroikaDataFile(data_fl)\n",
    "        ref=LoadTroikaRefFile(ref_fl)\n",
    "        features, labels, signals = [], [], []\n",
    "        left_s = (np.cumsum(np.ones(min(len(ppg), len(ref))) * fs * window_shift) - fs * window_shift).astype(int)\n",
    "        right_s = left_s + fs * window_length\n",
    "        for i in range(len(left_s)):\n",
    "            left, right = left_s[i], right_s[i]\n",
    "            ppg_w=BandpassFilter(ppg[left:right])\n",
    "            accx_w=BandpassFilter(accx[left:right])\n",
    "            accy_w=BandpassFilter(accy[left:right])\n",
    "            accz_w=BandpassFilter(accz[left:right])\n",
    "            features.append(extract_features(ppg_w, accx_w, accy_w, accz_w))\n",
    "            labels.append(ref[i])\n",
    "            signals.append([ppg_w, accx_w, accy_w, accz_w])\n",
    "    \n",
    "    features, labels = np.array(features), np.array(labels)\n",
    "    model = AdaBoostRegressor(n_estimators=400)\n",
    "    model.fit(features,labels)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def RunPulseRateAlgorithm(data_fl, ref_fl):\n",
    "    # Load data using LoadTroikaDataFile\n",
    "    ppg, accx, accy, accz = LoadTroikaDataFile(data_fl)\n",
    "    ref=LoadTroikaRefFile(ref_fl)\n",
    "    print(f\" len of ref ={len(ref)}\")\n",
    "    features, labels, signals = [], [], []\n",
    "    # Load ground truth heart rate\n",
    "    fs=125\n",
    "    window_length=8\n",
    "    window_shift=2 \n",
    "    left_s = (np.cumsum(np.ones(min(len(ppg), len(ref))) * fs * window_shift) - fs * window_shift).astype(int)\n",
    "    right_s = left_s + fs * window_length\n",
    "    for i in range(len(left_s)):\n",
    "        left, right = left_s[i], right_s[i]\n",
    "        ppg_w=BandpassFilter(ppg[left:right])\n",
    "        accx_w=BandpassFilter(accx[left:right])\n",
    "        accy_w=BandpassFilter(accy[left:right])\n",
    "        accz_w=BandpassFilter(accz[left:right])\n",
    "        features.append(extract_features(ppg_w, accx_w, accy_w, accz_w))\n",
    "        \n",
    "        labels.append(ref[i])\n",
    "        signals.append([ppg_w, accx_w, accy_w, accz_w])\n",
    "    \n",
    "    features, labels = np.array(features), np.array(labels)\n",
    "    print(f\" len of labels {len(labels)}\")\n",
    "    model= ensemble_model()\n",
    "    \n",
    "    # Compute pulse rate estimates and estimation confidence.\n",
    "    errors, confidence = [], []\n",
    "    \n",
    "    for i in range(len(signals)):\n",
    "        feature, label = features[i], labels[i]\n",
    "        ppg, accx, accy, accz = signals[i]\n",
    "        est = model.predict(np.reshape(feature, (1, -1)))[0]\n",
    "        \n",
    "        n = len(ppg) * 3\n",
    "        freqs = np.fft.rfftfreq(n, 1/fs)\n",
    "        fft = np.abs(np.fft.rfft(ppg,n))\n",
    "        fft[freqs <= 40/60.0] = 0.0\n",
    "        fft[freqs >= 240/60.0] = 0.0\n",
    "          # Max magnitude frequency\n",
    "        est_fs = est / 55.0\n",
    "        fs_win = 30  / 60.0\n",
    "        fs_win_e = (freqs >= est_fs - fs_win) & (freqs <= est_fs +fs_win)\n",
    "        conf = np.sum(fft[fs_win_e])/np.sum(fft)\n",
    "        \n",
    "        errors.append(np.abs((est-labels[i])))\n",
    "        confidence.append(conf)\n",
    "    print(f\" len of errors ={len(errors),len(confidence)} \\n \")\n",
    "\n",
    "    # Return per-estimate mean absolute error and confidence as a 2-tuple of numpy arrays.\n",
    "    return np.mean(errors), np.mean(confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " len of ref =148\n",
      " len of labels 148\n",
      " len of errors =(148, 148) \n",
      " \n",
      " len of ref =148\n",
      " len of labels 148\n",
      " len of errors =(148, 148) \n",
      " \n",
      " len of ref =140\n",
      " len of labels 140\n",
      " len of errors =(140, 140) \n",
      " \n",
      " len of ref =107\n",
      " len of labels 107\n",
      " len of errors =(107, 107) \n",
      " \n",
      " len of ref =146\n",
      " len of labels 146\n",
      " len of errors =(146, 146) \n",
      " \n",
      " len of ref =146\n",
      " len of labels 146\n",
      " len of errors =(146, 146) \n",
      " \n",
      " len of ref =150\n",
      " len of labels 150\n",
      " len of errors =(150, 150) \n",
      " \n",
      " len of ref =143\n",
      " len of labels 143\n",
      " len of errors =(143, 143) \n",
      " \n",
      " len of ref =160\n",
      " len of labels 160\n",
      " len of errors =(160, 160) \n",
      " \n",
      " len of ref =149\n",
      " len of labels 149\n",
      " len of errors =(149, 149) \n",
      " \n",
      " len of ref =143\n",
      " len of labels 143\n",
      " len of errors =(143, 143) \n",
      " \n",
      " len of ref =146\n",
      " len of labels 146\n",
      " len of errors =(146, 146) \n",
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14.585164226544865"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Project Write-up\n",
    "\n",
    "Answer the following prompts to demonstrate understanding of the algorithm you wrote for this specific context.\n",
    "\n",
    "> - **Code Description** \n",
    "Purpose of code is to build a  machine learning model trained to estmiate pulse from ppg and accelerometer data.\n",
    "This is a functional code consituting of the following functions:\n",
    "    * LoadTroikaDataset : loads the files containing our data\n",
    "    * LoadTroikaDataFile : attains the ppg and accelerometer signal data from the loaded files\n",
    "    * LoadTroikaRefFile : attains the reference data from loaded files \n",
    "    * BandpassFilter : performs bandpass filter to remove  undesirable frequencies outside the defined bandwidth in our function, the allowed bandwidth is from 40 to 240 bpm  ( notice it is divided by 60 to convert unit) \n",
    "    * extract_features: extract features from ppg and accleremoter data to be used in model , including peaks of fourier transform ( highest frequencies), mean of signal , and standard deviations of signals. Note the acceleromter magnitude is calculated before performing fourier transform also note the peaks are extracted using a heuritic approach in which the ppg peak should not match the acc peak\n",
    "    * ensemble_model : trains ensemble model , random forest, and trains it on given features and labels\n",
    "    * RunPulseRateAlgorithm : given files , loads signals and reference using LoadTroikaDataFile() and LoadTroikaRefFile() respecitively , it then performs windowing technique on all signals in which a window of 10 seconds is taken for every signal , features is extracted and labels is obtained from the reference array, eventually having features and labels arrays in which model is trained using ensemble_model(). Function then calculates errors(MAE) and confidence after predicting pulse using trained model and returns mean error array and mean confidence array \n",
    "    * Evaluate : given files,it loads files and uses RunPulseRateAlgorithm() to get errors and confidence arrays for all files and stacks them in one array\n",
    "    * AggregateErrorMetric : given stacked array of errors and confidence , it computes an aggregate error metric based on confidence estimates to return MAE at 90 % confidence\n",
    "\n",
    "> - **Data Description** \n",
    " * Data trained and tested on PPG and acceloremter data recorded from wearable device from 12 subjects during fast running at the peak speed of 15 km/h (note there is hidden extra test data that is why all data was used for training), the reference data or labels of our data was obtained from ECG \n",
    " * Data can be improved by having more subjects and by having more features about subjects , like age, gender, or smoking status\n",
    "> - **Algorithhm Description** will include the following:\n",
    ">   - Algorithm trains a random forest on  extracted features from ppg and acceleromter signals , features include , ppg maximum frequency that is different than accelormeter , accelerometer maximum frequency and means and std of ppg and acc signals\n",
    ">   - PPG sensor are light sensors that basically detect light shined from LEDS on the other side of the wrist, if perfusion is low , alot of of light will pass and the photodetector will read a hight signal.During ventricular sytole( which is basically the R wave in the ECG that is ideally used to measure pulse)  , blood is pumped from the heart to the peripheral ciruclation and thus pefusion at wrist increase, blood is a thick liquid with cells and plasma proteins so it blocks much light to photodetector, thus smaller signal. Using this concept we can estimate pulse from PPG , one problem though , perfusion of blood in wrist is also dependent on movement of the arm so data from accelormeter should be used to distinguish chnages of perfusion due to the cardiac cycle and changes due to the arm movement.\n",
    ">   - algorithim will like fail in extreme exercise or exercises very dependent on hand movement as it will become harder to distguish change in perfusion due to cardiac cycle\n",
    "> - **Algorithm Performance** - Model was trained without  a test set which might have made it less generalisable and this was due to the fact the data is relatively small, however a mean absolute error of 14.5\n",
    "Your write-up goes here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Next Steps\n",
    "You will now go to **Test Your Algorithm** to apply a unit test to confirm that your algorithm met the success criteria. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
